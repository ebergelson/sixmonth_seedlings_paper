---
title: "The Nature and Origins of the Lexicon in Six-month-olds"

# Use letters for affiliations, numbers to show equal authorship (if applicable) and to indicate the corresponding author
author:
  - name: Elika Bergelson
    affiliation: a,1
  - name: Richard Aslin
    affiliation: b, c
address:
  - code: a
    address: Duke University, Psychology & Neuroscience, 417 Chapel Drive, Durham, NC, 27708
  - code: b
    address: University of Rochester, Brain & Cognitive Sciences, Meliora Hall, Rochester, NY, 14627
  - code: c
    address: Haskins Laboratories, 300 George Street, New Haven, CT, 06511

corresponding_author:
  - code: 1
    text: "To whom correspondence should be addressed. E-mail: elika.bergelson@duke.edu"

# For footer text
lead_author_surname: Bergelson


author_contributions: |
 EB and RA developed the home-lab approach. EB and her staff conducted eyetracking and home data collection. EB performed data analysis and drafted the manuscript; RA provided feedback. Both approved the final submitted version.

abstract: |
  Recent research reported the surprising finding that even six-month-olds understand common nouns (Bergelson & Swingley, 2012 PNAS, 109, 3253-3258). But is their early lexicon structured and acquired like older learners'? We test six-month-olds for a hallmark of the mature lexicon: cross-word relations. We also examine whether properties of the home environment that have been linked with lexical knowledge in older children are detectable in the initial stage of comprehension. We use a new dataset, which includes in-lab comprehension and home measures from the same infants. We find evidence for cross-word structure: upon seeing two images of common nouns, infants’ looked significantly more at named target images when the competitor images were semantically unrelated (e.g. milk and foot) than when they were related (e.g. milk and juice), just as older learners do. We further find initial evidence for home-lab links: common noun "co-presence" (i.e. whether words’ referents were present and attended to in home recordings) correlated with in-lab comprehension. These findings suggest that even in neophyte word-learners, cross-word relations are formed early, and that the home learning environment measurably helps shape the lexicon from the outset.

significance: |
  Infants start understanding words at six months, when they also excel at subtle speech-sound distinctions and simple multi-modal associations, but don’t yet talk, walk, or point. However, true word-learning requires integrating the speech-stream with the world, and learning how words interrelate. Using eyetracking, we show that neophyte word learners already represent the semantic relations between words. We further show that these same infants’ word learning has ties to their environment:  the more they hear labels for what they’re looking at and attending to, the stronger their overall comprehension. These results provide a new integrative approach for investigating home-environment effects on early language, and suggest that language delays could be detected in early infancy for possible remediation.


acknowledgements: |
  We thank SEEDLingS staff: Amatuni, Dailey, Koorathota, Schneider, Tor, RAs at Rochester & Duke, and NIH T32 DC000035;DP5-OD019812(EB), & HD-037082 (RA & E. Newport).

keywords:
  - Word Learning
  - Lexicon
  - Cognitive Development
  - Language Acquisition
  - Environmental Effects

## must be one of: pnasresearcharticle (usual two-column layout), pnasmathematics (one column layout), or pnasinvited (invited submissions only)
pnas_type: pnasresearcharticle

bibliography: library06-12-17.bib
csl: pnas.csl

## change to true to add optional line numbering
lineno: false

output: 
  rticles::pnas_article:
    latex_engine: xelatex

nocite: | 
  @MacWhinney2000, @Dale1996, @Libertus2013, @Walle2014
---

```{r file_info, echo = F}
# Companion Data File for Bergelson & Aslin 2017, psych science
# This is the *manuscript* rmd for
# "Early (Noun) Relations"

# The eyetracking data were collected from infants in Rochester NY on a Eyelink 1000+
# The preprocessing script XX.r) precedes subsequent analysis and graphs
# The output of this pre-processing is a csv (XX.csv) 
# as well as a feather object (smaller/easier to work with) XX.feather.
# This pre-processed data file then gets read in by XX.r
# That's where data aggregation occurs
# This .rmd,  creates a doc output
# Please report any problems bugs or errors to Elika Bergelson (elika.bergelson@duke.edu)
#replace \begin{table} and \end{table with the below in the tex file 
#\begin{table*}[!th]
#\end{table*}

```

```{r setup, include = F}
# knitr::opts_chunk$set(
#   cache = F,  
#   fig.height = 4#, 
#   #fig.width = 16,
#   #out.width = '.8\\linewidth' 
#   )
knitr::opts_chunk$set(cache = F,
                      fig.height=3,
                      echo = FALSE,
                      message = FALSE,
                      warning = FALSE, 
                      cache = TRUE)#, 
                      #dev = 'cairo_pdf')
#knitr::opts_knit$get(kable.force.latex = TRUE)

library(tidyverse)
library(forcats)
library(feather)
library(stringr)
library(broom)
library(knitr)
library(boot)
library(irr)
library(kableExtra)

options(tibble.width = Inf)

options(scipen=999)

source(file = "sixmonth_only_stats_pnas.R")


```

To learn words, infants integrate their linguistic experiences with word-forms and the conceptual categories to which they refer. They do this fast: a growing literature demonstrates that by around 6 months, infants have begun understanding nouns [@Bergelson2012;@Bergelson2015b;@Parise2012;@Tincoff1999;@Tincoff2012], suggesting they form word-referent links from their environment in the first half-year. 

The speech-sound learning trajectory in year one is relatively well-established [@Gervain2010]: infants’ language-specific sensitivity emerges around 6 months for vowels, and 12 months for consonants [@Werker1984;@Polka1994]. Indeed, by 12 months, infants reveal robust phonetic representations for common words [@Swingley2002b;@Mani2010;@Vihman2004], and fine-grained knowledge of native language speech-sound combinatorics [@Swingley2009a]. Before this, their sensitivity to phonemic and talker-specific differences can be fragile [@Bergelson2017a;@Parise2012].

In contrast, early meaning is understudied: it's not clear what makes the first words infants understand learnable, or what aspects of meaning infants initially represent. This is partly because meaning components are not straightforward. While phonetic features (e.g. voicing) let us readily quantify speech-sound differences, characterizing meaning is harder; consider describing or comparing how 'dog' and 'log' sound versus what they mean. While toddlers are sensitive to visual similarity, shape, and semantic category [@Smith2002;@Wojcik2013a;@Borovsky2016;@Luche2014], little is known about nascent semantic representations.

Regarding early semantics, Arias-Trejo & Plunkett [-@Arias-Trejo2010] find that both visual similarity and category membership contribute to semantic competition: for toddlers, understanding ‘shoe’ in the context of a boot and a shoe was harder than when shoe appeared with a hat or bin instead. Thus, even in seasoned word learners, certain visual contexts make it harder to ascertain a spoken word's referent.

Bergelson & Aslin [@Bergelson2017] provide further data on early meanings. They find that over 12-20 months, infants’ semantic specificity increases: while younger infants looked at a named target to similar degrees whether hearing an appropriate or a related label (e.g. ‘cookie’ or ‘banana’ to label a cookie), older infants did so less. This suggests that one-year-olds have immature extensions for words they know something about (i.e. 'banana' could refer to a cookie), but leaves open whether younger infants’ are already sensitive to words' relatedness (i.e. banana and cookie's shared meaning).

Despite the relative dearth of infant work, a large literature reports on adults' single-word representations [@Neely1991, *inter alia*], revealing sensitivity to context and meaning. Adults consider semantic and perceptual relations among words in both the visual world paradigm [@Huettig2005;@Dahan2005], and in lexical decision tasks [@Neely1991]. Taken together, previous work with children and adults suggests that knowledge of how words are related goes hand-in-hand with knowledge of what words mean. Here we ask whether this is true for infants’ earliest words, or whether initial words are more like ‘islands,’ unrelated to other emerging lexical entries.

Although lab studies provide controlled assessments of children's knowledge, their natural habitats are far more complex. Corpus research has been crucial for establishing what children may learn *from* in their daily environments. Such data have been used to unpack both linguistic and non-linguistic aspects of the input [@Hart1992;@Weisleder2013;@Swingley2009a;@Hoff2002;@Shneidman2012;@Lieven1994]; they may also prove critical for understanding early lexical development.

However, there are exceedingly few available corpora of young infants, fewer yet with video, and none linking to comprehension measures in those same children. Here we begin to address these gaps by gathering real-time processing data for words within and across semantic categories, and investigating how infants’ home experiences with concrete nouns may influence their overall early comprehension. We ask:  1) Does semantic relatedness between visually available referents influence word comprehension in novice word learners?  2) Do readily-measurable aspects of infants’ home life account for *those same infants'* variability in word comprehension?  

We answer these questions through in-lab eyetracking and home recordings. The eyetracking experiment addressed question (1): we presented infants with image-pairs that were either semantically *related* or *unrelated* (e.g. car-stroller or car-juice); then, one image was named aloud (e.g. ‘car’). By hypothesis, if young infants are influenced by semantic relatedness (as toddlers and adults are), we predict better performance in the unrelated trials. I.e., in the context of two semantically unrelated images, we predict stronger comprehension than in the context of two related images.

Our home environment analysis addressed question (2): we gathered daylong audio and hour-long video recordings from infants in their homes, and examined just those time-slices when concrete nouns were directed to the infant. We then derived measures of quantity, talker-variability, utterance-type and situational context, and explored how they might be related to performance on the eyetracking task. 

Previous research with older infants suggests that hearing more words strengthens the early vocabulary whether in daily interactions [@Weisleder2013], or in shared reading [@Debaryshe1993;@Montag2015]. In the lab, talker-variability aids word-learning [@Rost2009]. For utterance-type, words said in short phrases (i.e. alone or at utterance edges) are proposed to be learned more readily [@Brent2001;@Seidl2006]. Relatedly, syntactic diversity (i.e. hearing a word across more sentence structures) has been shown to aid word-learning [@Hoff2002]. 

Finally, we examined referential transparency (i.e. whether parents talk about observable referents), which is suggested to facilitate language learning [@Medina2011;@Yurovsky2013;@McGillion2013]. Notably, previous research examining why infants learn common non-nouns (e.g. 'hi', 'eat') later than common nouns found that visual co-presence varied across word-class [@Bergelson2013]. Non-nouns were more likely to be said when the referent event was not occurring (i.e. "hi" said with no-one entering the scene) vis-à-vis nouns, which were generally proximally present when named. If such referential transparency were a basic feature that boosts learnability, then here too we would expect the degree of ‘object co-presence’ in infants' experience would map onto early comprehension, which has not previously been shown *within* infants, or *within* nouns.

#Results {#results .unnumbered}
Data processing and annotation information, and in-house scripts are available on our OSF lab wiki and github repository.^[https://osf.io/cxwyz/, https://github.com/SeedlingsBabylab/] Raw home recording data (audio and video) are available through HomeBank and Databrary, respectively; see details in Methods; clips available in Supporting Information (SI) online. 

## Eyetracking Results{#eyetrackingresults .unnumbered}
Eyetracking data were processed in R 3.3.1 to determine where the child was looking for each 20ms bin during the test trials: the target or distractor interest areas (an invisible 620x620 pixel rectangle around each image), or neither.  Eye movement data were time-aligned to parents’ target word utterances (noted by experimenter key-press). 

```{r f-tt, fig.height=2, fig.width=2, message = F, echo = F, warning = F,  fig.cap="\\label{fig:f-tt}Comprehension by Trial-Type. Dots represents each infant's baseline-corrected proportion of target looking, per trial-type (unrelated, related). Mean and 95% CIs are in black. Asterisk indicates p<.05 for the unrelated and related trial-types; the fraction indicates the proportion of infants with positive trial-type means"}

ggplot(sixmonthseedlings_proptTT_Ss06 %>% filter(!subj%in%missingTT_Ss$subj),
         aes(trialtype, cross_item_mean_proptcorrTT, color = trialtype, alpha=trialtype))+
  geom_point(size=3, shape = 1)+
  scale_alpha_manual(values = c(.2, .5),guide=F)+
  stat_summary(fun.data=mean_cl_normal, color = "black", 
               geom = "pointrange", size = .75, fatten =1.5, alpha = 1)+
  geom_hline(yintercept = 0)+
  theme_bw(base_size = 8)+
  xlab("Trial Type")+
  ylab("baseline-corrected \nprop. target looking")+
  scale_x_discrete(labels= c("unrelated","related"))+
  annotate("segment", x=1,xend=2, y= .37, yend=.37, size = .35)+
  annotate("text",label = "*", x = 1.5, y = .34, size = 4)+
  annotate("segment", x=.9,xend=1.1, y= .34, yend=.34, size = .35)+
  annotate("text",label = "*", x = 1, y = .34, size = 4)+
  annotate("text",label = "26/51", x = 2, y = -.3, size = 3)+
  annotate("text",label = "36/51", x = 1, y = -.3, size = 3)+
  scale_color_manual(values = c("blue","darkgrey"),guide=F)+
  scale_shape_discrete(solid=T,guide=F)
```

These data were then aggregated across two time-windows: a pre-target baseline from trial start to target word onset, and a post-target window from 367ms to trial end, i.e. 5000ms after target onset. Given the longitudinal home-and-lab design of this data collection, we exclude at the trial level rather than the infant level, where possible. Trials were excluded if infants did not look at either image for at least 1/3 of the target window (367-5000ms), if no looking was recording in the pre-target baseline window, or if the trial was never displayed due to experiment termination for infant fussiness; see SI. 

```{r shapiro-wilk test, echo = F, message = F,results = "hide"}
shap_proptcorr_p <- shapiro.test((sixmonthseedlings_propt_Ss06 %>% filter(!subj %in% missingTT_Ss$subj))$cross_item_mean_proptcorr)$p.value
```

We used the standard baseline-corrected target-looking metric, which calculates the proportion of target looking (target/(target+distractor)) in the post-target window, and subtracts away this same proportion from the baseline window. Visual inspection of subject means revealed one outlier (>5SD above the mean). Once this outlier was removed, this outcome measure did not differ from a normal distribution (Shapiro-Wilk Test, p = `r round(shap_proptcorr_p,2)`.).

```{r eyetracking only stats, echo = F}
#of the 58 infants on the overall spreadsheet, the excluded subjects are as follows:
#ns06, ns08, 25, 36: zero usable trials (already not in spreadsheet of et data)
#05,06: only trials in ONE trial-type (missingTT_Ss)
#23: outlier (>5sd above the mean; removed as outlier)

unrelated_ttest<-t.test(filter(sixmonthseedlings_proptTT_Ss06,
                     !subj %in%missingTT_Ss$subj &
                       trialtype=="between")$cross_item_mean_proptcorrTT)%>%
  tidy()
related_ttest <-t.test(filter(sixmonthseedlings_proptTT_Ss06,
                     !subj %in%missingTT_Ss$subj &
                       trialtype=="within")$cross_item_mean_proptcorrTT)%>%
  tidy()

et_trialtype_ttest <- t.test(data = filter(sixmonthseedlings_proptTT_Ss06, !subj %in%missingTT_Ss$subj), 
                             cross_item_mean_proptcorrTT~trialtype, paired=T)%>%
  tidy()
unrelated_Ss <- sixmonthseedlings_proptTT_Ss06 %>%
  filter(!subj %in%missingTT_Ss$subj)%>%
  filter(trialtype=="between")%>%
    summarise(numpos = sum(cross_item_mean_proptcorrTT>0),
            numinfants = n(),
            proppos = str_c(numpos,"/",numinfants),
            meanproptcorr = mean(cross_item_mean_proptcorrTT, na.rm=T),
            sdproptcorr = sd(cross_item_mean_proptcorrTT, na.rm=T),
            mean_sd_p_proptcorr = str_c("M=",round(meanproptcorr,3),
                                        ", SD=",round(sdproptcorr,3),
                                        ", p = ",round(binom.test(numpos, numinfants)$p.value,3)))

related_Ss <- sixmonthseedlings_proptTT_Ss06 %>%
  filter(!subj %in%missingTT_Ss$subj)%>%
  filter(trialtype=="within")%>%
    summarise(numpos = sum(cross_item_mean_proptcorrTT>0),
            numinfants = n(),
            proppos = str_c(numpos,"/",numinfants),
            meanproptcorr = mean(cross_item_mean_proptcorrTT, na.rm=T),
            sdproptcorr = sd(cross_item_mean_proptcorrTT, na.rm=T),
            mean_sd_p_proptcorr = str_c("M=",round(meanproptcorr,3),
                                        ", SD=",round(sdproptcorr,3),
                                        ", p = ",round(binom.test(numpos, numinfants)$p.value,3)))
overall_ttest<-t.test(filter(sixmonthseedlings_propt_Ss06,
                             !subj %in%missingTT_Ss$subj & 
        !startsWith(as.character(subj),"ns"))$cross_item_mean_proptcorr)%>%
  tidy()

```

```{r item stats, echo = F, message = F, warning = F, results = "hide"}
shapiro.test(sixmonthseedlings_proptTT_items06$cross_item_mean_proptcorrTT)
item_props <- sixmonthseedlings_proptTT_items06 %>%
  group_by(trialtype) %>% 
    summarise(numpos = sum(cross_item_mean_proptcorrTT>0),
            numitems = n(),
            proppos = str_c(numpos,"/",numitems),
            meanitems = mean(cross_item_mean_proptcorrTT, na.rm=T),
            sditems = sd(cross_item_mean_proptcorrTT, na.rm=T),
            mean_sd_proptcorr_items = str_c("M=",round(meanitems,3),
                                        ", SD=",round(sditems,3)))

unrelated_ttest_items <- t.test(subset(sixmonthseedlings_proptTT_items06,trialtype=="between")$cross_item_mean_proptcorrTT,
            conf.int=T)%>%
  tidy()
related_ttest_items <- t.test(subset(sixmonthseedlings_proptTT_items06,trialtype=="within")$cross_item_mean_proptcorrTT,
            conf.int=T)%>%
  tidy()
et_trialtype_ttest_items <- t.test(data = sixmonthseedlings_proptTT_items06, 
            cross_item_mean_proptcorrTT~trialtype,
            conf.int=T)%>%tidy()
```

We predicted that if infants’ comprehension is affected by semantic relatedness, then performance on related trials would be worse than on unrelated trials. Indeed, performance was significantly above chance on unrelated trials (t(`r unrelated_ttest$parameter`) = `r round(unrelated_ttest$statistic,2)`, p = `r round(unrelated_ttest$p.value,3)`, by 2-tailed 1-sample T-test), at chance on related trials (t(`r related_ttest$parameter`) = `r round(related_ttest$statistic,2)`, p = `r round(related_ttest$p.value,3)`, *ibid*), and significantly different between the two (t(`r et_trialtype_ttest$parameter`) = `r round(et_trialtype_ttest$statistic,2)`, p = `r round(et_trialtype_ttest$p.value,3)`, by 2-tailed paired T-test). That is, infants looked more at the labeled image on unrelated trials than on related trials. Over subjects, `r unrelated_Ss$proppos` infants attained positive subject means for the unrelated trial-type (`r unrelated_Ss$mean_sd_p_proptcorr` by binomial test), while only `r related_Ss$proppos` did so for related trials (`r related_Ss$mean_sd_p_proptcorr` by binomial test). Over items, infants showed the same numeric pattern as over subjects, i.e. the item-mean was higher for unrelated items than related items (`r item_props$mean_sd_proptcorr_items[1]`, `r item_props$mean_sd_proptcorr_items[2]`, respectively), and item-means were positive for most items in the unrelated condition, but not the related condition; with only sixteen items, item-level effects were not different from chance (p>.05). Summarily, in the related condition but not unrelated condition, performance was positive over most infants and most items; see Figures \ref{fig:f-tt}, S1 and S2. 


```{r reliability, echo=F, results = "hide", message=F}
videorel%>% 
  mutate(utrel = ifelse(orig_utt_type==new_utt_type, T, F),
         oprel = ifelse(orig_present==new_present, T, F),
          orig_present = factor(orig_present),
         new_present = factor(new_present),
         orig_utt_type = factor(orig_utt_type),
         new_utt_type = factor(new_utt_type))%>%
  summary()

kappavidUT<- kappa2(videorel[,c("orig_utt_type","new_utt_type")])
kappavidOP<-kappa2(videorel[,c("orig_present","new_present")])
agreevidUT<-agree(videorel[,c("orig_utt_type","new_utt_type")])
agreevidOP<-agree(videorel[,c("orig_present","new_present")])

audiorel%>% 
  mutate(utrel = ifelse(orig_utt_type==new_utt_type, T, F),
         oprel = ifelse(orig_present==new_present, T, F),
         orig_present = factor(orig_present),
         new_present = factor(new_present),
         orig_utt_type = factor(orig_utt_type),
         new_utt_type = factor(new_utt_type))%>%
  #filter(utrel==F | oprel==F) %>% #write_csv(.,"audio_rel_withagreement.csv")
  summary(maxsum=50)

kappaaudUT<- kappa2(audiorel[,c("orig_utt_type","new_utt_type")])
kappaaudOP<-kappa2(audiorel[,c("orig_present","new_present")])
agreeaudOP<-agree(audiorel[,c("orig_present","new_present")])
agreeaudUT<-agree(audiorel[,c("orig_utt_type","new_utt_type")])


meankappaOP <- round(mean(c(kappaaudOP$value,kappavidOP$value)),1)
meankappaUT <- round(mean(c(kappaaudUT$value,kappavidUT$value)),1)
meanagreeOP <- round(mean(c(agreeaudOP$value, agreevidOP$value),2))
meanagreeUT <- round(mean(c(agreeaudUT$value, agreevidUT$value),2))

```

## Home Recording Results {#home-rec-res .unnumbered}
After pre-processing (see Methods), annotators marked each object word (i.e. concrete noun) in the recordings along with three properties: utterance-type, object co-presence, and speaker. Each word’s utterance-type was classified by its syntactic and prosodic features into 7 categories: declaratives, questions, imperatives, short-phrases, reading, singing, and unclear.  Object co-presence was coded ‘yes’, ‘no’, and ‘unclear’ based on whether the annotator felt that the object corresponding to the word being annotated was present and attended to by the child. For videos this was generally visually appreciable; for audio-files, annotators used their impression from the context (e.g. generally, in "here’s your spoon!", spoon was coded 'yes' for object co-presence, while in "twinkle little star", star was coded 'no'). Inter-rater reliability was high (computed for 10% of annotations for utterance-type: `r meanagreeUT`% agreement, Cohen's $K$=`r meankappaUT` and object co-presence: `r meanagreeOP`% agreement, Cohen's $K$=`r meankappaOP`). We then derived token-counts for the various forms a word occurred in (e.g. "tooth,","tootheroo," "teeth"), and type-counts for the lemma (e.g. "tooth").

```{r f-ut, fig.height=1.25, fig.width=3.25,message = F, echo = F, warning = F,  fig.cap="\\label{fig:f-ut}Distribution of object-word utterances, by infant. Color shows utterance-type (daylong audio left, hour-long video right.) 1 video was lost; 1 word with unclear utterance-type removed. X-axes are identically ordered by each child's overall proportion of object-words in declaratives and questions"}

#made a new df to arrange the order the x-axis in the barchart by increasing propd+prodq

# and to make fill with d&q at the bottom & take out the 1 unknown utterance type

ut_by_propdq_df <- sixmonth_basiclevel_home_data_agg%>%
  dplyr::select(subj, ent_subj_av,propd, audio_video, propq)%>%
  left_join(sixmonth_basiclevel_home_data)%>%
  group_by(subj)%>%
  mutate(propdq = propd+propq)%>%
  arrange(propdq)%>%
  ungroup()%>%
  mutate(subj_sorted = fct_inorder(subj),
         utterance_type_rev = fct_rev(utterance_type))%>%
  filter(utterance_type!="u")


ggplot(ut_by_propdq_df, aes(subj_sorted, fill = utterance_type_rev))+
  geom_bar(position = "fill")+
  #facet_wrap(~audio_video, scales = "free_y", nrow = 1)+
  facet_grid(~audio_video)+
  scale_fill_discrete(limits = c("i","n","r","s","q","d"),
                      labels=c("imperative", "short phrase", "reading","singing","question","declarative"))+
  theme_bw(base_size=6)+
  xlab("individual infants")+
  ylab("proportion of input\nby utterance-type")+
  guides(fill=guide_legend(title=NULL))+
  theme(legend.position="right",legend.text = element_text(size = 5),
        axis.text.x = element_blank(),
        legend.key.height=unit(.3,"cm"),
        legend.key.width=unit(.1,"cm"),
        legend.margin=margin(0,0,0,0),
        legend.box.margin=margin(-10,0,-5,-10))
```

While input data varied by child, there was relatively high consistency in our proportional measures; see Table \ref{tab:t-homestats}. We operationalized input quantity as the number of types and tokens of each lemma. Given that the video recordings were all approximately 1 hour, while the audio recordings varied based on the nap-time of the child, we report *daily* audio rates and *hourly* video rates.

In our daylong audio recordings, infants heard ~710 object-word tokens of ~180 word-types, from 7 speakers, on average. 60% of this input came from infants' mothers, and 50% of the time that infants heard an object-word, the corresponding referent was visible and attended to (i.e. 50% 'object co-presence'). In our hour-long video recordings, infants heard ~170 tokens, from ~60 types, from 3 speakers. In the videos, 70% of the input came from infants' mothers, with 60% object co-presence. For both audio and video recordings, the average entropy across utterance types (i.e. the variability proportions of each utterance type in 'bits') was 1.9; short phrases were 10% of the input (See Fig. \ref{fig:f-ut}).

```{r t-homestats, echo = F}
tableAV <- kable(rbind(audio_homestats, video_homestats),
      col.names = c("","# tokens","# types","# speakers","prop mat input","prop obj.co-p.","utt type ent", "prop short phrase"),
      caption = "Home Recording Descriptive Statistics: Range, Mean(SD)", longtable = F, format = "latex")

tableAV  %>%
  kable_styling(latex_options = c("scale_down")) %>% 
  #kable_styling(full_width=T, font = 3)%>%
  add_footnote(c("aud=audio, vid=video, prop=proportion, mat = maternal, obj.co-p = object co-presence, utt= utterance, ent = entropy"), notation = "alphabet")
# simpler_table %>% 
#   kable(digits = 1, col.names = seq(1:29),
#       caption = "Descriptive statistics from home recordings (Range, Mean, and SD)", longtable = F, format = "latex")

```

Comparing the daylong audio- and hour-long video-recordings, we found that while infants heard more object-word input in the audio-recordings in an absolute sense, they heard *relatively* more input in the videos. That is, infants heard only 25-50% fewer word-tokens, word-types, and speakers in the hour-long video than in the daylong audio recording (from a different day), even though the latter was ~10-11x longer than the former; we explore this in ongoing work.

## Questionnaires {#quests .unnumbered}
Vocabulary questionnaires (MCDI) showed that parents felt their infants understood few of our 16 tested words (M=1.96 (3.98), R: 0–15, mode: 0), while on Word Exposure Surveys parents indicated that their child heard these words daily on average (4 on a 1-5 scale from 'never' to 'several times a day'). According to parental report, 71% of infants were not yet babbling, all but one were not yet hands-and-knees crawling; 29% were exclusively breast-fed; see SI. 

```{r home lab stats, echo=F,results = "hide", warning = F, message = F,fig.show="hide"}
#our eyetracking outcome measure is normal
shapiro.test(audsixmonth$cross_item_mean_proptcorr)
shapiro.test(vidsixmonth$cross_item_mean_proptcorr)

#audio only: r and numspeakers are not normal
shapiro.test(audsixmonth$numtypes)
shapiro.test(audsixmonth$numtokens)
shapiro.test(audsixmonth$type_token_ratio)
shapiro.test(audsixmonth$r)#not normal
shapiro.test(audsixmonth$numspeakers)# not normal
#audio: quantity and outcome: some marginal, one sig.
# numtypes and proptcorr is the one that's just barely significant
audtypes_proptcorr_cor <-
  cor.test(audsixmonth$cross_item_mean_proptcorr,
           audsixmonth$numtypes) %>%
  tidy()
audtokens_proptcorr_cor <-
  cor.test(audsixmonth$cross_item_mean_proptcorr,
           audsixmonth$numtokens) %>% tidy()#marginal

audr_proptcorr_Kcor <- cor.test(audsixmonth$cross_item_mean_proptcorr,
  audsixmonth$r,
  method = "kendall") %>% tidy()#marginal

audttr_proptcorr_cor <- cor.test(audsixmonth$cross_item_mean_proptcorr,
           audsixmonth$type_token_ratio) %>% tidy()
audnumsp_proptcorr_Kcor <- cor.test(audsixmonth$cross_item_mean_proptcorr,
  audsixmonth$numspeakers,
  method = "kendall") %>% tidy()
  
#audio: quantity measures all correlated significantly with each other
a<-cor.test(audsixmonth$r, audsixmonth$numtokens, method = "kendall")$estimate#.40
a1<-cor.test(audsixmonth$r, audsixmonth$numtokens, method = "kendall")$p.value
b<-cor.test(audsixmonth$r, audsixmonth$numtypes, method = "kendall")$estimate#.38
b1<-cor.test(audsixmonth$r, audsixmonth$numtypes, method = "kendall")$p.value
c<-cor.test(audsixmonth$numtokens, audsixmonth$numtypes)$estimate#.82
c1<-cor.test(audsixmonth$numtokens, audsixmonth$numtypes)$p.value
d<-cor.test(audsixmonth$numtokens, audsixmonth$type_token_ratio)$estimate# -.82
d1<-cor.test(audsixmonth$numtokens, audsixmonth$type_token_ratio)$p.value#
e<-cor.test(audsixmonth$numtypes, audsixmonth$type_token_ratio)$estimate# -.49
e1<-cor.test(audsixmonth$numtypes, audsixmonth$type_token_ratio)$p.value#
f<-cor.test(audsixmonth$r, audsixmonth$type_token_ratio, method = "kendall")$estimate# -.3
f1<-cor.test(audsixmonth$r, audsixmonth$type_token_ratio, method = "kendall")$p.value

#video: all but type token ratio are not normal
shapiro.test(vidsixmonth$numtypes)# not normal
shapiro.test(vidsixmonth$numtokens)# not normal
shapiro.test(vidsixmonth$type_token_ratio)
shapiro.test(vidsixmonth$r)#not normal
shapiro.test(vidsixmonth$numspeakers)#not normal
#video: nothing significantly correlates with in-lab performance
vidtypes_proptcorr_cor<-cor.test(vidsixmonth$cross_item_mean_proptcorr,
                                 vidsixmonth$numtypes) %>% tidy()
vidtokens_proptcorr_Kcor <- cor.test(vidsixmonth$cross_item_mean_proptcorr,
                                     vidsixmonth$numtokens, method = "kendall") %>% tidy()
vidttr_proptcorr_cor<-cor.test(vidsixmonth$cross_item_mean_proptcorr,
                               vidsixmonth$type_token_ratio) %>% tidy()
vidr_proptcorr_cor<-cor.test(vidsixmonth$cross_item_mean_proptcorr,
                             vidsixmonth$r) %>% tidy()
vidnumsp_proptcorr_Kcor<-cor.test(vidsixmonth$cross_item_mean_proptcorr, 
                                  vidsixmonth$numspeakers, method = "kendall") %>% tidy()

#video: quantity measures all correlated significantly, except type token and r
g <- cor.test(vidsixmonth$r, vidsixmonth$numtokens, method = "kendall")$estimate#.55
g1 <- cor.test(vidsixmonth$r, vidsixmonth$numtokens, method = "kendall")$p.value#.
h<-cor.test(vidsixmonth$r, vidsixmonth$numtypes, method = "kendall")$estimate#.57
h1<-cor.test(vidsixmonth$r, vidsixmonth$numtypes, method = "kendall")$p.value#.
i<- cor.test(vidsixmonth$numtokens, vidsixmonth$numtypes, method = "kendall")$estimate#.79
i1<- cor.test(vidsixmonth$numtokens, vidsixmonth$numtypes, method = "kendall")$p.value#
j<-cor.test(vidsixmonth$numtokens, vidsixmonth$type_token_ratio, method = "kendall")$estimate#-.49
j1<-cor.test(vidsixmonth$numtokens, vidsixmonth$type_token_ratio, method = "kendall")$p.value#
k<-cor.test(vidsixmonth$numtypes, vidsixmonth$type_token_ratio, method = "kendall")$estimate#-.27
k1<-cor.test(vidsixmonth$numtypes, vidsixmonth$type_token_ratio, method = "kendall")$p.value#
l<-cor.test(vidsixmonth$r, vidsixmonth$type_token_ratio, method = "kendall")$estimate#-.23
l1<-cor.test(vidsixmonth$r, vidsixmonth$type_token_ratio, method = "kendall")$p.value#

roundmincor <- round(min(abs(c(a,b,c,d,e,f,g,h,i,j,k,l))),2)
roundmaxcor <- round(max(abs(c(a,b,c,d,e,f,g,h,i,j,k,l))),2)

#combined aud/vid home measures for proportions: prop_mom not normal
shapiro.test(propvarssixmonth$prop_op_avg)
shapiro.test(propvarssixmonth$prop_mom_avg)# not normal
shapiro.test(propvarssixmonth$ent_subj_avg)
shapiro.test(propvarssixmonth$prop_n_avg)# not normal
#prop_op  significantly correlated with outcome
propop_proptcorr_corr <- cor.test(propvarssixmonth$cross_item_mean_proptcorr,
                                  propvarssixmonth$prop_op_avg)%>%tidy()


bothmom_proptcorr_Kcor<-cor.test(propvarssixmonth$cross_item_mean_proptcorr, 
                                 propvarssixmonth$prop_mom_avg, method = "kendall") %>% tidy()
bothent_proptcorr_cor<-cor.test(propvarssixmonth$cross_item_mean_proptcorr,
                                propvarssixmonth$ent_subj_avg) %>% tidy()
bothpropn_proptocor_cor<-cor.test(propvarssixmonth$cross_item_mean_proptcorr,
                                  propvarssixmonth$prop_n_avg) %>% tidy()

```

```{r f-propop-proptcorr, echo = F, warning = F, fig.height=1.5, fig.width=2.5, fig.cap="\\label{fig:f-propop-proptcorr}In-lab Comprehension by Proportion of Object Co-Presence. Each point indicates a given infant's average proportion of object co-presence in the audio and video home recordings (x-axis) by that same infant's subject mean in the eyetracking experiment (y-axis). Line indicates robust linear fit with 95% CI in grey"}

ggplot(propvarssixmonth,
       aes(prop_op_avg,cross_item_mean_proptcorr))+
  geom_point()+
  stat_smooth(method = "rlm") +
  xlab("prop. object co-presence")+
  ylab("baseline-corrected \nsubj. mean comprehension")+
  theme_bw(base_size=7)+
  geom_hline(yintercept = 0)+
  annotate(geom = "text", label = "r = .39, p = .013", x = .4, y = .25,color = "red", size = 2.5)

```

## Home and Lab Linkages  {#home-lab-links .unnumbered}
We next examined data from the infants who provided both home and in-lab data (video and eyetracking: n=40; audio and eyetracking: n=41). We modeled infants’ subject means from our eyetracking experiment as a function of the properties we had annotated (noun input, talker, utterance-types, and object co-presence). Given the relatively small sample size, the large number of ways one might aggregate the home data, and both predicted and unanticipated collinearity among these four pre-identified properties of interest, we opted for a simple analysis approach. Namely, we tested directionally-specified correlations between home-environment measures found to predict lexical knowledge in previous research, and infants' in-lab comprehension. That is, if measures that predict lexical knowledge in older infants hold at 6 months at levels our home measures can detect, we would see positive correlations between in-lab comprehension and noun input, talker variability, utterance-type diversity, and object co-presence. When analyzing counts, we examined audio and video data separately since length varied substantially by recording-type. When examining proportions, we averaged audio and video data. We first conducted Shapiro-Wilk Normality tests; if both variables were normally distributed, we used Pearson correlations; if one or both were not, we used Kendall correlations.

As described above, the eyetracking results suggest that neophyte word learners are sensitive to semantic similarity.  However, given that the home environment is not split into experiences relevant for our two experimental trial-types, we examine how *overall* lab performance correlates with home measures. Notably, aggregating across trial-types for each infant, performance overall was not above chance given the relatively weak performance on related trials (t(`r overall_ttest$parameter`) = `r round(overall_ttest$statistic,2)`, p = `r round(overall_ttest$p.value,3)`.)^[This t-test was run on the subset of infants reported above for whom there is also home recording data (n=41); the same pattern holds over all infants included in the initial analysis] 

We find a significant correlation between the proportion of object co-presence and infants' overall in-lab comprehension (r= `r round(propop_proptcorr_corr$estimate,2)`, p=`r round(propop_proptcorr_corr$p.value,3)`). See Fig. \ref{fig:f-propop-proptcorr}. This result is consistent with a broader role for referential transparency in word-learning, even *among* (early-learned) nouns; we return to this below. 

We next considered four measures of object-word quantity from the literature: number of types, tokens, and words read, along with type-token ratio. These variables tend to be highly correlated with each other and indeed we find each of them significantly correlated with the others in both audio and video data (|r| between `r roundmincor`-`r roundmaxcor`; `r ifelse((max(c(a1,b1,c1,d1,e1,f1,g1,h1,i1,j1,k1,l1)))<.05,"ps<.05","fix")`).^[As expected, type-token ratio *negatively* correlated with quantity; all other rs were positive] 

Given predicted correlations among quantity measures, we asked how these measures correlated with lab performance. No video quantity measures correlated significantly with in-lab comprehension. However, several quantity metrics from the audio recordings (number of types, tokens, and object-word containing reading utterances) were close to the p<.05 significance threshold (r = `r round(audtypes_proptcorr_cor$estimate,2)`, p = `r round(audtypes_proptcorr_cor$p.value,3)`, r = 
`r round(audtokens_proptcorr_cor$estimate,2)`,p = `r round(audtokens_proptcorr_cor$p.value,3)`, and
$\tau$ = `r round(audr_proptcorr_Kcor$estimate,2)`, p = `r round(audr_proptcorr_Kcor$p.value,3)`, respectively); this leaves open the possibility that an expanded home and lab sample, or other measures of word knowledge and/or input quantity would render clearer results.

Looking at talker variability, we again did not find significant correlations with in-lab comprehension in audio or video recordings, or in the proportion of input from infants' mothers (all p>.05). Finally, looking at utterance-type variability (calculated as each infant's average entropy across utterance-types), we found no significant correlations with in-lab comprehension (p=`r round(bothent_proptcorr_cor$p.value,2)`), likely due to highly similar utterance-type distributions across participants (See Fig. \ref{fig:f-ut}.) We replicated previous work reporting that infants hear ~10% of words in isolation [@Brent2001], but this property too was not associated with in-lab comprehension (p=`r round(bothpropn_proptocor_cor$p.value,2)`).

```{r bootstrap,  echo=F,results = "hide", warning = F, message = F}
# boostrap CIs for corr prop_op & proptcorr
minidat_bootstrap <- filter(propvarssixmonth, !is.na(cross_item_mean_proptcorr))
pearson_eb <- function(d,i=c(1:length(minidat_bootstrap$cross_item_mean_proptcorr))){
  d2 <- d[i,]
  return(cor(d2$cross_item_mean_proptcorr,d2$prop_op_avg))
}
bootcorr_eb <- boot(data = dplyr::select(ungroup(minidat_bootstrap), cross_item_mean_proptcorr, prop_op_avg), statistic = pearson_eb, R = 1000)
#hist(bootcorr_eb$t)
#plot(ecdf(bootcorr_eb$t))
bootci_eb <- boot.ci(bootcorr_eb, conf=.95)

aud_vid_propop<- audsixmonth %>% 
  ungroup() %>% 
  filter(subj%in%vidsixmonth$subj) %>% 
  rename(aprop_op = prop_op) %>% 
  dplyr::select(subj, aprop_op) %>% 
  bind_cols(dplyr::select(vidsixmonth,prop_op)) %>% 
  rename(vprop_op = prop_op) 
shapiro.test(aud_vid_propop$vprop_op)
shapiro.test(aud_vid_propop$aprop_op)
aud_vid_propop <- cor.test(aud_vid_propop$aprop_op, aud_vid_propop$vprop_op, conf.int=T)

#bootstrap CIs for corr numtypes & proptcorr
minidat_bootstrap_audtypes <- filter(audsixmonth, !is.na(cross_item_mean_proptcorr))
pearson_eb_aud <- function(d,i=c(1:length(minidat_bootstrap_audtypes$cross_item_mean_proptcorr))){
  d2 <- d[i,]
  return(cor(d2$cross_item_mean_proptcorr,d2$numtypes))
}
bootcorr_eb_aud <- boot(data = dplyr::select(ungroup(minidat_bootstrap_audtypes), cross_item_mean_proptcorr, numtypes), statistic = pearson_eb_aud, R = 1000)
#hist(bootcorr_eb_aud$t)
#plot(ecdf(bootcorr_eb_aud$t))
bootci_eb_aud <- boot.ci(bootcorr_eb_aud, conf=.95)


```

Thus, across the four aspects of home environment we predicted would positively correlate with in-lab comprehension, only object co-presence clearly did so, though it is premature to conclude that this variable is a differentially better predictor than the others. Given that our analyses were predicated upon directional predictions, but served as initial exploratory steps, we evaluated this correlation further by calculating bootstrapped confidence intervals (CIs) with 1000 iterations. The 95% CI did not include 0 (.15-.62).^[Audio and video object co-presence were marginally correlated with each other, r=`r round(aud_vid_propop$estimate,2)`, p=`r round(aud_vid_propop$p.value,3)`]

#Discussion {#discussion .unnumbered}
Consistent with research on adults and children, we find that 6-month-olds understand words more readily when shown two semantically unrelated referents than when shown two related ones. We further find initial evidence that in these same infants, in-lab word comprehension is linked with referential transparency in the home, but not with measures of talker or utterance-type, and only marginally with input quantity. These findings enrich our understanding of infants' real-time word comprehension, and the longer-scale learning environment that fuels it.

Our eyetracking results suggest that even first words are not unconnected islands of meaning; they already contain semantic structure. Still, there are various interpretations of infants’ relatively strong performance on unrelated trials (e.g. car-juice), versus their poor performance on related trials (e.g. car-stroller). One possibility is that infants know (something about) the tested words, but cannot overcome semantic competition on related trials, i.e. hearing "car" leads to car-looking, but also activates related words, e.g. stroller, to a similar (or indistinguishable) degree. 

Alternatively, infants’ word knowledge may be underspecified: they may know enough about a word's meaning to tell it apart from the unrelated referent, but not the related one (which by design has distributional, conceptual and/or visual overlap). I.e., perhaps infants know "car" cannot refer to juice, but not whether stroller is in the "car" category. 

Furthermore, these options may intertwine, and indeed, our timecourse data are compatible with both (Figure S2): infants consistently looked at the labeled target image in related trials, but shifted between the images in unrelated trials, consistent with underspecification or competition.

In studies of children and adults, these possibilities are disambiguated by both 'cleaner' (i.e. less noisy, more accurate) eye movements, which reveal transient looks at the semantically-related distractor, and overt (touch or click) target selection, which 6-month-olds (who do not even point yet) cannot do.  Additional infant measures (e.g. neural recordings or reaching tasks) may be promising future directions.

These results also complement previous early word comprehension research [@Bergelson2012;@Bergelson2015b;@Tincoff1999;@Tincoff2012]. Here too, further work may elucidate how presentation method influences infants’ looking behavior (e.g. video vs. photo; 2-image displays vs. scenes).

Turning to the corpus results, home-lab links were relatively limited. Specifically, we did not find a relation between how infants' object-word input was distributed across speakers and utterance-types, and infants’ comprehension of common nouns. Especially for utterance-type, this may reflect the limits of analyzing only object-word utterances. Similarly, input quantity measures (which are tied to toddlers' vocabulary, e.g. [@Weisleder2013]) were only marginally correlated with comprehension. While these variables may matter for subsequent lexical knowledge, they did not do so here. Given our young and relatively homogenous sample, we may have had limited variability therein.

Intriguingly, object co-presence significantly correlated with in-lab comprehension. Of course, infants do not learn words and referents they have not experienced. Furthermore, parents' ‘focusing in’ on infants’ attention likely provides higher-quality learning instances [@McGillion2013]. Our results also suggest an expansion of Bergelson & Swingley [-@Bergelson2013], who found that common nouns are more referentially transparent than common non-nouns. Here we suggest that referential transparency for a given infant may map onto that *same child's* comprehension, within the already referentially-transparent noun class.

These results are first steps toward understanding how the initial lexicon is organized and acquired from experience by 6 months. We find that real-time comprehension is influenced by links among words. We further find promising results tying infants' experiences with referential transparency to early word knowledge. Finally, these results suggest a combined lab-home approach can begin to reveal the range and dynamics of learner-by-environment interaction, as infants' initial understanding of words starts to give way to robust knowledge of their language.

```{r trial excl det, results = "hide", eval = F, echo = F}
#for safekeeping in case we decide to put it back in:
#Trials were excluded from analysis if infants did not look at either image for at least 1/3 of the target window of analysis (367-5000ms; `r #nrow(lowdatatrials)-nrow(nopre_andlow)` trials), if no looking was recording in the pre-target baseline window (`r nrow(trials_win_no_pre)` trials),  or if the trial #was never displayed due to the experiment ending earlier because of infant fussiness (`r sum(subswhostoppedearly$missedtrials_endedearly)`).

```

#Materials and Methods {#matmethods .unnumbered}
## Participants {#participants .unnumbered}
The final eyetracking experiment sample was 51 6-month-olds (M = 6.1mo, R = 5.6-6.7mo, 23 female). Families elected to participate in a one-time lab-only study (n=12; LO group), or additionally enroll in a larger yearlong study with home visits (n=44; HL group). Four further infants who participated in the eyetracking study were excluded for fussiness or calibration failure resulting in 0 trials with sufficient data for analysis, in one or both trial-types. One additional excluded infant performed >5SD above the mean. See SI for further details on per-trial exclusion criteria.

Infants were recruited from a database in Rochester, NY. All children were healthy, had no hearing or vision problems, were carried full-term (40 $\pm$ 3 weeks), and heard $\geq$ 75% English at home. Families received \$10 and a small gift for the lab study, and a further $5 if they also completed the home visits (audio and video). Families who completed our optional demographics questionnaire (98%) reported that infants were largely white and middle class (HL: 95% white, 75% of mothers received a B.A. or higher; LO: 79% & 57%, respectively).

Given unknown effect sizes for home-lab links in young infants, the target sample size was 48, i.e. 3x the standard minimum sample size (n=16). We enrolled and retained 44 infants over an 8mo. enrollment window for the HL group. Once HL enrollment ended, LO did as well. 

```{r f-itempairs, echo = F,  message = F, fig.cap="\\label{fig:f-itempairs}Item-Pairs in Eyetracking Study. Infants saw each image-pair twice. There were 16 trials in each trial-type (related, unrelated), 32 trials total"}
#fig1 <- knitr::include_graphics("images/sixmonth_pairs_figure-2.eps")
fig1 <- knitr::include_graphics("images/fig1_wide.eps")
fig1
```

## Lab Visit Procedure{#labvisproc .unnumbered}
First, staff explained the study to families and obtained consent for either the yearlong study (including home recordings) or one-time eyetracking, as relevant (approved by the U.Rochester IRB). Parents then completed surveys about their child; see SI.

Next, the parent sat with the infant in their lap in a dimly lit testing room, in front of an Eyelink 1000+ Eyetracker, which was in head-free mode, and sampled monocularly at 500 Hz (<0.5$^{\circ}$ average accuracy; SR Research, Ontario, Canada). A small sticker on infant's forehead tracked head movements. The experiment was run from a computer that was back-to-back with the testing monitor, allowing for adjustment if the child moved out of eyetracking range.

The experiment began with four "warm-ups", in which a single image was labeled by a sentence played over speakers (e.g. "Look at the apple!") Parents were then given a visor that blocked the screen (or closed their eyes, n=3), and over-ear headphones. The experiment was also recorded by camcorder to ensure compliance and monitor the child’s state.

Next came 32 test trials, in which infants saw two images on a grey background, and heard a sentence labeling one image; See Fig.\ref{fig:f-itempairs}. An attention-getter was shown as needed. On each test trial, parents spoke a single sentence aloud to their child, which labeled one of the images on the screen; they first heard a prerecorded sentence over headphones that they then repeated aloud [@Bergelson2012]. Images were shown for 5s after target word onset; the length of time before the parent said the target word after the images appeared varied across trials, averaging ~3–4s.

Each infant saw both trial-types (16 related and 16 unrelated trials, interspersed pseudo-randomly). Infants were alternately assigned to two trial orders, which counterbalanced side and ordering of images, target items, and trial-type.

### Stimuli{#stimuli .unnumbered}
16 common concrete nouns were chosen as target words based on corpora and prior research, see SI and Fig.\ref{fig:f-itempairs}. Each word was part of two item-pairs, one in each trial-type (related and unrelated, e.g. dog-baby and spoon-baby; n=16 item-pairs). Pairings maximized semantic overlap within related pairs, and minimized it in unrelated pairs. Semantic network analyses confirmed that related pairs were more similar than unrelated pairs; see SI. Within trial-type, items were paired to minimize phonetic overlap.^[1 related pair unavoidably had the same initial consonant: book-ball.] 

Audio stimuli were sentences recorded by a female using infant-directed speech prosody in a sound-booth; they were 1.1-1.8s, normalized to 72db (i.e. a volume that allowed only parents to hear them when they were played over the parent headphones). Infants only heard the sentences from their parents (see Procedure). Each sentence occurred in 1 of 4 carrier phrases: "Can you find the X?", "Where’s the X?", "Do you see the X?", and "Look at the X!", where X is the target word (only 1 sentence frame was used per item-pair).

Visual stimuli were photos of each target and warm-up word, edited onto a grey background, displayed at 500x500 pixels on a 27.4 by 34-cm LCD 96 PPI screen, at a viewing distance of 55-60cm. Warm-up images (n=4) were displayed centrally. For test-trials (n=32), each image was centered within the left and right half of the screen (counterbalanced across trials). Each of 16 test photos occurred 4 times: once as target and as distractor in each trial-type (related and unrelated; see Fig.\ref{fig:f-itempairs}.).

## Home Recording Procedure{#homerecproc .unnumbered}
Home recordings captured infants’ typical environment through an hour-long video recording, and, on a separate day, a daylong audio recording.  Before recording, parents were given a release form which allowed up to 3 levels of sharing; see SI. Recordings that parents opted to share with authorized researchers can be found on Databrary and Homebank (n=43); access is available to researchers who complete ethics certification and membership agreements through these repositories. See sample clips in SI.

Video recordings took place at infants’ homes. Research staff put a specialized hat on the child, and a small Looxcie camera (8.4x1.7x1.3cm, 22g) was affixed above each ear with Velcro (one pointed slightly up and one slightly down, to better capture the child’s visual field). Staff also put a camcorder on a tripod in the corner. Parents were given an information sheet, asked to move the tripod if they changed rooms, and given staff contacts. Staff left and returned after 1 hour.

At either the lab or home visit parents were given a LENA audio recorder (8.6x5.6x1.3cm, 57g) and clothing with a LENA-pocket. (LENA Foundation, Boulder, CO). Parents were asked to turn the recorder on when the child awoke, and off at the end of the day or if they wanted to stop recording. Recording time ranged from 10.66 to 16.00 hours (M=14.37).

<!-- Leave these lines as they are at the end of your .Rmd file to ensure placement of methods & acknowledgements sections before the references-->
\showmatmethods
\showacknow 
<!-- \pnasbreak -->
\newline
